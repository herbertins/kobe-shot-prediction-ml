# ğŸ€ Kobe Shot Prediction

Este projeto usa tÃ©cnicas de Machine Learning e engenharia de dados para prever se um arremesso de Kobe Bryant foi **cesta ou erro**, com base em informaÃ§Ãµes contextuais da jogada.

---

## ğŸ“Š Etapas do projeto

| Etapa                          | DescriÃ§Ã£o |
|-------------------------------|-----------|
| **AquisiÃ§Ã£o dos dados**       | ImportaÃ§Ã£o dos arquivos `.parquet` com as jogadas |
| **Processamento**             | RemoÃ§Ã£o de dados nulos, seleÃ§Ã£o de variÃ¡veis |
| **DivisÃ£o treino/teste**      | SeparaÃ§Ã£o estratificada (80/20) |
| **Treinamento**               | Modelos: regressÃ£o logÃ­stica e Ã¡rvore de decisÃ£o |
| **AvaliaÃ§Ã£o**                 | MÃ©tricas: Log Loss e F1-Score (registradas no MLFlow) |
| **Deploy**                    | Salvamento e aplicaÃ§Ã£o do modelo final |
| **Dashboard**                 | Interface com Streamlit para anÃ¡lise em produÃ§Ã£o |

---

## ğŸ“Š Dashboard de Monitoramento

Dashboard criado com Streamlit para monitorar os resultados em produÃ§Ã£o.

### Como executar:

```bash
streamlit run dashboard.py
```

VocÃª poderÃ¡ visualizar:

- âœ… Taxa de acerto
- ğŸ“‰ DistribuiÃ§Ã£o das previsÃµes
- ğŸ” Amostras das prediÃ§Ãµes feitas
- ğŸ§  Interface leve e responsiva

---

## ğŸ› ï¸ Como executar o projeto localmente

1. Clone o repositÃ³rio
2. Crie um ambiente virtual com Python 3.10+
3. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

4. Execute o pipeline completo:

```bash
kedro run
```

5. Inicie o dashboard:

```bash
streamlit run dashboard.py
```

---

## ğŸ“š ReferÃªncias

- Framework TDSP (Microsoft)
- PyCaret 3.x
- Kedro docs
- MLFlow docs
- Dataset Kobe Bryant [original no Kaggle](https://www.kaggle.com/datasets/kobe24/kobe-bryant-shot-selection)

---

## ğŸ’¼ Sobre este projeto

### 2. Iremos desenvolver um preditor de arremessos usando duas abordagens (regressÃ£o e classificaÃ§Ã£o) para prever se o "Black Mamba" (apelido de Kobe) acertou ou errou a cesta.

## ğŸ”§ Arquitetura do Pipeline

![Pipeline Diagram](docs/kobe_diagram.png)

> ğŸ“ O diagrama acima foi criado com [draw.io](https://app.diagrams.net).  
> VocÃª pode editÃ¡-lo no arquivo [`docs/diagram.drawio`](docs/diagram.drawio)

## ğŸ“ Estrutura do projeto

Projeto desenvolvido seguindo o **Framework TDSP da Microsoft** com foco em boas prÃ¡ticas de MLOps e reprodutibilidade com Kedro.

```
kobe-shot-prediction-ml/
â”œâ”€â”€ conf/                      # ConfiguraÃ§Ãµes de pipeline e parÃ¢metros (por ambiente)
â”œâ”€â”€ data/                      # DiretÃ³rios organizados por estÃ¡gios do pipeline
â”‚   â”œâ”€â”€ 01_raw/                # Dados brutos (originais)
â”‚   â”œâ”€â”€ 02_intermediate/       # Dados processados parcialmente
â”‚   â”œâ”€â”€ 03_primary/            # Dados limpos, estruturados
â”‚   â”œâ”€â”€ 04_features/           # Dados com features engenheiradas
â”‚   â”œâ”€â”€ 05_model_input/        # Base pronta para treinamento
â”‚   â”œâ”€â”€ 06_models/             # Modelos treinados (.pkl)
â”‚   â”œâ”€â”€ 07_model_output/       # PrediÃ§Ãµes e inferÃªncias
â”‚   â””â”€â”€ 08_reporting/          # RelatÃ³rios finais, comparaÃ§Ãµes e visualizaÃ§Ãµes
â”‚
â”œâ”€â”€ docs/                      # DocumentaÃ§Ã£o e diagramas do projeto
â”œâ”€â”€ logs/                      # Logs do Kedro
â”œâ”€â”€ mlruns/                    # DiretÃ³rio de tracking do MLflow
â”œâ”€â”€ notebooks/                 # Notebooks auxiliares ou exploratÃ³rios
â”œâ”€â”€ src/kobe_shot_prediction_ml/
â”‚   â”œâ”€â”€ nodes/                 # FunÃ§Ãµes puras (transformaÃ§Ãµes, treinos, prediÃ§Ãµes)
â”‚   â”œâ”€â”€ pipelines/             # DefiniÃ§Ã£o e orquestraÃ§Ã£o das pipelines
â”‚   â”œâ”€â”€ pipeline_registry.py   # Registro central das pipelines
â”‚   â”œâ”€â”€ settings.py            # ConfiguraÃ§Ãµes globais do projeto
â”‚   â””â”€â”€ dashboard.py           # (Opcional) scripts para visualizaÃ§Ãµes ou dashboards
â”‚
â”œâ”€â”€ pyproject.toml             # ConfiguraÃ§Ã£o de dependÃªncias e metadata do projeto
â”œâ”€â”€ requirements.txt           # DependÃªncias adicionais
â””â”€â”€ README.md                  # DocumentaÃ§Ã£o principal do projeto
```

### 3. Como as ferramentas Streamlit, MLflow, PyCaret e Scikit-Learn auxiliam na construÃ§Ã£o dos pipelines?

## ğŸš€ Tecnologias utilizadas

- [Kedro](https://kedro.readthedocs.io/en/stable/) â€“ OrquestraÃ§Ã£o de pipelines
- [PyCaret](https://pycaret.org/) â€“ Treinamento automatizado de modelos
- [MLFlow](https://mlflow.org/) â€“ Rastreamento de experimentos e deployment
- [Scikit-learn](https://scikit-learn.org/) â€“ MÃ©tricas e suporte a modelos
- [Streamlit](https://streamlit.io/) â€“ Dashboard interativo
- [Pandas, Parquet, PyArrow] â€“ ManipulaÃ§Ã£o de dados

## âš™ï¸ Tecnologias e FunÃ§Ãµes no Pipeline de Machine Learning

Este projeto utiliza ferramentas modernas para construir, rastrear, atualizar e disponibilizar modelos de machine learning de ponta a ponta. Abaixo estÃ¡ a descriÃ§Ã£o do papel de cada uma delas no pipeline:

### ğŸ§© Ferramentas Utilizadas

| Ferramenta     | FunÃ§Ã£o Principal                                               |
|----------------|----------------------------------------------------------------|
| **Scikit-learn** | Base para modelos, mÃ©tricas e transformaÃ§Ãµes tradicionais     |
| **PyCaret**      | AbstraÃ§Ã£o para automaÃ§Ã£o de experimentos e pipelines de ML    |
| **MLflow**       | Rastreabilidade, versionamento e gerenciamento de modelos     |
| **Streamlit**    | Interface interativa para visualizaÃ§Ã£o, validaÃ§Ã£o e deploy    |

---

### ğŸ” Como cada ferramenta auxilia no pipeline

#### **a. Rastreamento de Experimentos** â€“ *MLflow + PyCaret*
- Armazena os parÃ¢metros utilizados nos experimentos (`test_size`, `model_type`)
- Registra mÃ©tricas como `log_loss`, `f1_score`, etc.
- Permite comparar execuÃ§Ãµes anteriores e promover o melhor modelo
- PyCaret jÃ¡ loga automaticamente os experimentos no MLflow

#### **b. FunÃ§Ãµes de Treinamento** â€“ *PyCaret + Scikit-learn*
- PyCaret automatiza o fluxo: `setup`, `create_model`, `tune_model`, `predict_model`
- Scikit-learn Ã© usado para mÃ©tricas manuais, validaÃ§Ã£o e nodes personalizados
- Ambos sÃ£o integrados aos pipelines do Kedro

#### **c. Monitoramento da SaÃºde do Modelo** â€“ *MLflow + Streamlit*
- MLflow mantÃ©m histÃ³rico de mÃ©tricas e modelos
- Permite detectar degradaÃ§Ã£o de performance entre versÃµes
- Streamlit pode exibir dashboards com mÃ©tricas, distribuiÃ§Ã£o de prediÃ§Ãµes e alertas

#### **d. AtualizaÃ§Ã£o de Modelo** â€“ *PyCaret + Kedro + MLflow*
- Novos dados podem ser passados para o `setup()` do PyCaret
- O pipeline gera um novo modelo, avalia, e registra uma nova versÃ£o no MLflow
- O Kedro automatiza esse processo mantendo o controle de artefatos

#### **e. Provisionamento (Deployment)** â€“ *Streamlit + MLflow + Scikit-learn*
- Streamlit permite criar uma interface para usuÃ¡rios testarem o modelo
- MLflow pode servir o modelo via REST API
- Modelos `.pkl` treinados com Scikit-learn ou PyCaret podem ser integrados em APIs, dashboards ou sistemas externos

---

### 4. Com base no diagrama realizado na questÃ£o 2, aponte os artefatos que serÃ£o criados ao longo de um projeto. Para cada artefato, a descriÃ§Ã£o detalhada de sua composiÃ§Ã£o.

## ğŸ“¦ Artefatos Gerados no Pipeline

Abaixo estÃ£o os artefatos criados ao longo das etapas do pipeline, conforme representado no diagrama:

---

### ğŸ”¹ Pipeline: `data_processing`

#### `data_filtered.parquet`
- **DescriÃ§Ã£o**: Base original (`dataset_kobe_dev.parquet`) apÃ³s limpeza e filtragem de colunas/linhas irrelevantes.
- **ComposiÃ§Ã£o**:
  - Colunas relevantes para o modelo
  - Sem valores nulos ou inconsistentes
  - Pode conter dados balanceados ou amostrados

---

### ğŸ”¹ Pipeline: `data_split`

#### `base_train.parquet`
- **DescriÃ§Ã£o**: Conjunto de dados usado para **treinamento** dos modelos.
- **ComposiÃ§Ã£o**:
  - Subconjunto da `data_filtered.parquet`
  - Separado com base no parÃ¢metro `test_size`
  - Inclui variÃ¡veis preditoras e a variÃ¡vel alvo

#### `base_test.parquet`
- **DescriÃ§Ã£o**: Conjunto de dados usado para **validaÃ§Ã£o/teste** dos modelos.
- **ComposiÃ§Ã£o**:
  - Complementar ao `base_train.parquet`
  - Utilizado para cÃ¡lculo de mÃ©tricas de performance

---

### ğŸ”¹ Pipeline: `model_training`

#### `lr_model.pkl`
- **DescriÃ§Ã£o**: Modelo de regressÃ£o logÃ­stica treinado.
- **ComposiÃ§Ã£o**:
  - Objeto serializado `.pkl` com o modelo e transformaÃ§Ãµes
  - Exportado via PyCaret ou Scikit-learn

#### `dt_model.pkl`
- **DescriÃ§Ã£o**: Modelo de Ã¡rvore de decisÃ£o treinado.
- **ComposiÃ§Ã£o**:
  - Mesmo formato do `lr_model.pkl`, treinado com outro algoritmo

#### `prediction_model_lr.parquet`
- **DescriÃ§Ã£o**: PrediÃ§Ãµes geradas pelo modelo de regressÃ£o logÃ­stica.
- **ComposiÃ§Ã£o**:
  - Colunas: `shot_made_flag`, `prediction_label`, `prediction_score`, etc.

#### `prediction_model_dt.parquet`
- **DescriÃ§Ã£o**: PrediÃ§Ãµes geradas pelo modelo de Ã¡rvore de decisÃ£o.
- **ComposiÃ§Ã£o**:
  - Mesmo formato do arquivo anterior

#### `final_model.pkl`
- **DescriÃ§Ã£o**: Modelo selecionado como o melhor (menor `log_loss`).
- **ComposiÃ§Ã£o**:
  - Modelo final aprovado para uso em produÃ§Ã£o
  - Pode ser o `lr_model` ou `dt_model`

---

### ğŸ”¹ Pipeline: `AplicaÃ§Ã£o de modelo`

#### `dataset_kobe_prod.parquet`
- **DescriÃ§Ã£o**: Resultado da aplicaÃ§Ã£o do `final_model.pkl` em dados de produÃ§Ã£o.
- **ComposiÃ§Ã£o**:
  - Dados originais de produÃ§Ã£o + prediÃ§Ãµes
  - Ex: `prediction_label`, `score`, `id`, etc.

---

### 5. SeparaÃ§Ã£o entre treino e teste e impacto no modelo final

Separar bem os dados de treino e teste Ã© uma etapa importante pra garantir que o modelo funcione de verdade e nÃ£o sÃ³ "decore" os dados. Se a gente usa um conjunto de treino que Ã© muito diferente do teste, o modelo pode parecer bom durante o desenvolvimento, mas falhar quando for colocado pra rodar de verdade.

Pra evitar esse tipo de problema, algumas prÃ¡ticas ajudam bastante:
- Dividir os dados de forma equilibrada (mantendo a proporÃ§Ã£o entre acertos e erros, por exemplo)
- Embaralhar os dados antes de separar em treino e teste
- Usar validaÃ§Ã£o cruzada pra testar o modelo em vÃ¡rias divisÃµes diferentes
- Analisar os dados antes de treinar, pra ver se estÃ£o bem distribuÃ­dos

#### Sobre os dados:
- O dataset original tinha por volta de **24.271 registros** e **25 colunas**
- Depois que removemos os registros sem informaÃ§Ã£o no `shot_made_flag`, ficaram **20.285 registros** e **7 colunas**
- Esses dados foram salvos no arquivo: `data/processed/data_filtered.parquet`

---

### 6. Pipeline de Treinamento com MLflow (`Treinamento`)

ApÃ³s comparar as mÃ©tricas de ambos os modelos (log loss e F1), o modelo com **melhor desempenho em log loss** foi escolhido como modelo final. Essa escolha foi feita automaticamente no pipeline por meio de um node de seleÃ§Ã£o. 

Durante o treinamento, tanto o modelo de regressÃ£o logÃ­stica quanto o de Ã¡rvore de decisÃ£o apresentaram **mÃ©tricas muito prÃ³ximas**. Isso levantou uma dÃºvida: qual mÃ©trica usar para escolher o melhor modelo?

Decidi usar o **log loss** como critÃ©rio principal, porque:

- Ele nÃ£o olha sÃ³ se o modelo acertou ou errou, mas **tambÃ©m leva em conta a confianÃ§a da prediÃ§Ã£o**
- Um erro com **muita certeza** (por exemplo, prever que a chance era 95% e errar) Ã© penalizado mais fortemente
- Isso faz o log loss ser uma mÃ©trica mais exigente e justa quando trabalhamos com **probabilidades** ao invÃ©s de sÃ³ rÃ³tulos

Mesmo que dois modelos tenham um `f1_score` parecido, o log loss nos ajuda a entender **quem estÃ¡ mais calibrado** nas suas previsÃµes.

---

### 7. Pipeline de AplicaÃ§Ã£o com MLflow (`PipelineAplicacao`)

## ğŸ“ˆ AnÃ¡lise de AderÃªncia do Modelo em ProduÃ§Ã£o

**Resultados observados:**

- `F1 Score (produÃ§Ã£o)`: **0.34**
- `Log Loss (produÃ§Ã£o)`: **16.44**

**O modelo **nÃ£o Ã© aderente** Ã  nova base:**

Esses valores indicam que o modelo teve dificuldade em generalizar para a nova distribuiÃ§Ã£o dos dados.  
DiferenÃ§as em `lat`, `lon` e demais variÃ¡veis indicam um possÃ­vel **data drift** em relaÃ§Ã£o Ã  base de treino.

**Justificativa**

O modelo foi treinado com uma base que provavelmente apresenta **outras posiÃ§Ãµes de arremesso (lat/lon) e outras condiÃ§Ãµes de jogo.**
Ao ser aplicado em uma base nova com caracterÃ­sticas diferentes, **ele estÃ¡ fazendo previsÃµes com alta confianÃ§a em situaÃ§Ãµes que ele nÃ£o conhece bem**, o que leva ao aumento do erro (log loss alto) e queda no F1.

## ğŸ” Monitoramento da SaÃºde do Modelo e EstratÃ©gias de Retreinamento

### Monitoramento da saÃºde do modelo (com e sem variÃ¡vel de resposta)

| CenÃ¡rio | EstratÃ©gia de Monitoramento |
|--------|------------------------------|
| **Com variÃ¡vel de resposta** | - Calcular mÃ©tricas como `f1_score`, `log_loss`, `accuracy`<br>- Comparar prediÃ§Ãµes com rÃ³tulos reais<br>- Atualizar dashboards com mÃ©tricas de performance |
| **Sem variÃ¡vel de resposta** | - Monitorar distribuiÃ§Ã£o dos dados de entrada (ex: `lat`, `lon`, etc.)<br>- Detectar *data drift* com testes estatÃ­sticos (ex: PSI, KS Test)<br>- Observar mudanÃ§as no padrÃ£o das prediÃ§Ãµes (prediction drift)<br>- Validar presenÃ§a de valores nulos, outliers, entradas inesperadas |

---

### EstratÃ©gias de retreinamento do modelo

| EstratÃ©gia | DescriÃ§Ã£o |
|------------|-----------|
| **Reativa** | Ocorre quando Ã© identificada uma degradaÃ§Ã£o na performance do modelo (ex: queda no `f1_score`, aumento no `log_loss`). Um novo modelo Ã© treinado com dados mais recentes. |
| **Preditiva** | O retreinamento ocorre de forma agendada (ex: mensal, trimestral), mesmo que o modelo ainda esteja performando bem. Previne obsolescÃªncia causada por mudanÃ§as lentas nos dados. |

> Ambas as estratÃ©gias podem ser combinadas em um pipeline de MLOps automatizado com validaÃ§Ãµes e alertas.

## âœ¨ InspiraÃ§Ã£o

> *"The most important thing is to try and inspire people so that they can be great in whatever they want to do."*  
> â€” Kobe Bryant
